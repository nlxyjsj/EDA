{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4736,
     "status": "ok",
     "timestamp": 1732368432944,
     "user": {
      "displayName": "张殿",
      "userId": "10927409075531378782"
     },
     "user_tz": -480
    },
    "id": "-qaAD0dgX3r4",
    "outputId": "9ad27c51-61b5-4dd1-95a2-0175b9f92187"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:177: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:285: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:177: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<>:285: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "<ipython-input-1-50759020fe5c>:177: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  words = [word for word in words if word is not '']\n",
      "<ipython-input-1-50759020fe5c>:285: SyntaxWarning: \"is not\" with a literal. Did you mean \"!=\"?\n",
      "  words = [word for word in words if word is not '']\n"
     ]
    }
   ],
   "source": [
    "# Easy data augmentation techniques for text classification\n",
    "# Jason Wei, Chengyu Huang, Yifang Wei, Fei Xing, Kai Zou\n",
    "\n",
    "import random\n",
    "from random import shuffle\n",
    "random.seed(1)\n",
    "\n",
    "#stop words list\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our',\n",
    "\t\t\t'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "\t\t\t'yourself', 'yourselves', 'he', 'him', 'his',\n",
    "\t\t\t'himself', 'she', 'her', 'hers', 'herself',\n",
    "\t\t\t'it', 'its', 'itself', 'they', 'them', 'their',\n",
    "\t\t\t'theirs', 'themselves', 'what', 'which', 'who',\n",
    "\t\t\t'whom', 'this', 'that', 'these', 'those', 'am',\n",
    "\t\t\t'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "\t\t\t'have', 'has', 'had', 'having', 'do', 'does', 'did',\n",
    "\t\t\t'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n",
    "\t\t\t'because', 'as', 'until', 'while', 'of', 'at',\n",
    "\t\t\t'by', 'for', 'with', 'about', 'against', 'between',\n",
    "\t\t\t'into', 'through', 'during', 'before', 'after',\n",
    "\t\t\t'above', 'below', 'to', 'from', 'up', 'down', 'in',\n",
    "\t\t\t'out', 'on', 'off', 'over', 'under', 'again',\n",
    "\t\t\t'further', 'then', 'once', 'here', 'there', 'when',\n",
    "\t\t\t'where', 'why', 'how', 'all', 'any', 'both', 'each',\n",
    "\t\t\t'few', 'more', 'most', 'other', 'some', 'such', 'no',\n",
    "\t\t\t'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too',\n",
    "\t\t\t'very', 's', 't', 'can', 'will', 'just', 'don',\n",
    "\t\t\t'should', 'now', '']\n",
    "\n",
    "#cleaning up text\n",
    "import re\n",
    "def get_only_chars(line):\n",
    "\n",
    "    clean_line = \"\"\n",
    "\n",
    "    line = line.replace(\"’\", \"\")\n",
    "    line = line.replace(\"'\", \"\")\n",
    "    line = line.replace(\"-\", \" \") #replace hyphens with spaces\n",
    "    line = line.replace(\"\\t\", \" \")\n",
    "    line = line.replace(\"\\n\", \" \")\n",
    "    line = line.lower()\n",
    "\n",
    "    for char in line:\n",
    "        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n",
    "            clean_line += char\n",
    "        else:\n",
    "            clean_line += ' '\n",
    "\n",
    "    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n",
    "    if clean_line[0] == ' ':\n",
    "        clean_line = clean_line[1:]\n",
    "    return clean_line\n",
    "\n",
    "########################################################################\n",
    "# Synonym replacement\n",
    "# Replace n words in the sentence with synonyms from wordnet\n",
    "########################################################################\n",
    "\n",
    "#for the first time you use wordnet\n",
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def synonym_replacement(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\trandom_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "\trandom.shuffle(random_word_list)\n",
    "\tnum_replaced = 0\n",
    "\tfor random_word in random_word_list:\n",
    "\t\tsynonyms = get_synonyms(random_word)\n",
    "\t\tif len(synonyms) >= 1:\n",
    "\t\t\tsynonym = random.choice(list(synonyms))\n",
    "\t\t\tnew_words = [synonym if word == random_word else word for word in new_words]\n",
    "\t\t\t#print(\"replaced\", random_word, \"with\", synonym)\n",
    "\t\t\tnum_replaced += 1\n",
    "\t\tif num_replaced >= n: #only replace up to n words\n",
    "\t\t\tbreak\n",
    "\n",
    "\t#this is stupid but we need it, trust me\n",
    "\tsentence = ' '.join(new_words)\n",
    "\tnew_words = sentence.split(' ')\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "def get_synonyms(word):\n",
    "\tsynonyms = set()\n",
    "\tfor syn in wordnet.synsets(word):\n",
    "\t\tfor l in syn.lemmas():\n",
    "\t\t\tsynonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "\t\t\tsynonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "\t\t\tsynonyms.add(synonym)\n",
    "\tif word in synonyms:\n",
    "\t\tsynonyms.remove(word)\n",
    "\treturn list(synonyms)\n",
    "\n",
    "########################################################################\n",
    "# Random deletion\n",
    "# Randomly delete words from the sentence with probability p\n",
    "########################################################################\n",
    "\n",
    "def random_deletion(words, p):\n",
    "\n",
    "\t#obviously, if there's only one word, don't delete it\n",
    "\tif len(words) == 1:\n",
    "\t\treturn words\n",
    "\n",
    "\t#randomly delete words with probability p\n",
    "\tnew_words = []\n",
    "\tfor word in words:\n",
    "\t\tr = random.uniform(0, 1)\n",
    "\t\tif r > p:\n",
    "\t\t\tnew_words.append(word)\n",
    "\n",
    "\t#if you end up deleting all words, just return a random word\n",
    "\tif len(new_words) == 0:\n",
    "\t\trand_int = random.randint(0, len(words)-1)\n",
    "\t\treturn [words[rand_int]]\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "########################################################################\n",
    "# Random swap\n",
    "# Randomly swap two words in the sentence n times\n",
    "########################################################################\n",
    "\n",
    "def random_swap(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\tfor _ in range(n):\n",
    "\t\tnew_words = swap_word(new_words)\n",
    "\treturn new_words\n",
    "\n",
    "def swap_word(new_words):\n",
    "\trandom_idx_1 = random.randint(0, len(new_words)-1)\n",
    "\trandom_idx_2 = random_idx_1\n",
    "\tcounter = 0\n",
    "\twhile random_idx_2 == random_idx_1:\n",
    "\t\trandom_idx_2 = random.randint(0, len(new_words)-1)\n",
    "\t\tcounter += 1\n",
    "\t\tif counter > 3:\n",
    "\t\t\treturn new_words\n",
    "\tnew_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n",
    "\treturn new_words\n",
    "\n",
    "########################################################################\n",
    "# Random addition\n",
    "# Randomly add n words into the sentence\n",
    "########################################################################\n",
    "\n",
    "def random_addition(words, n):\n",
    "\tnew_words = words.copy()\n",
    "\tfor _ in range(n):\n",
    "\t\tadd_word(new_words)\n",
    "\treturn new_words\n",
    "\n",
    "def add_word(new_words):\n",
    "\tsynonyms = []\n",
    "\tcounter = 0\n",
    "\twhile len(synonyms) < 1:\n",
    "\t\trandom_word = new_words[random.randint(0, len(new_words)-1)]\n",
    "\t\tsynonyms = get_synonyms(random_word)\n",
    "\t\tcounter += 1\n",
    "\t\tif counter >= 10:\n",
    "\t\t\treturn\n",
    "\trandom_synonym = synonyms[0]\n",
    "\trandom_idx = random.randint(0, len(new_words)-1)\n",
    "\tnew_words.insert(random_idx, random_synonym)\n",
    "\n",
    "########################################################################\n",
    "# main data augmentation function\n",
    "########################################################################\n",
    "\n",
    "def eda_4(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=4):\n",
    "\n",
    "\tsentence = get_only_chars(sentence)\n",
    "\twords = sentence.split(' ')\n",
    "\twords = [word for word in words if word is not '']\n",
    "\tnum_words = len(words)\n",
    "\n",
    "\taugmented_sentences = []\n",
    "\tnum_new_per_technique = int(num_aug/4)+1\n",
    "\tn_sr = max(1, int(alpha_sr*num_words))\n",
    "\tn_ri = max(1, int(alpha_ri*num_words))\n",
    "\tn_rs = max(1, int(alpha_rs*num_words))\n",
    "\n",
    "\t#sr\n",
    "\tfor _ in range(num_new_per_technique):\n",
    "\t\ta_words = synonym_replacement(words, n_sr)\n",
    "\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\t#ri\n",
    "\tfor _ in range(num_new_per_technique):\n",
    "\t\ta_words = random_addition(words, n_ri)\n",
    "\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\t#rs\n",
    "\tfor _ in range(num_new_per_technique):\n",
    "\t\ta_words = random_swap(words, n_rs)\n",
    "\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\t#rd\n",
    "\tfor _ in range(num_new_per_technique):\n",
    "\t\ta_words = random_deletion(words, p_rd)\n",
    "\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\taugmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences]\n",
    "\tshuffle(augmented_sentences)\n",
    "\n",
    "\t#trim so that we have the desired number of augmented sentences\n",
    "\tif num_aug >= 1:\n",
    "\t\taugmented_sentences = augmented_sentences[:num_aug]\n",
    "\telse:\n",
    "\t\tkeep_prob = num_aug / len(augmented_sentences)\n",
    "\t\taugmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n",
    "\n",
    "\t#append the original sentence\n",
    "\taugmented_sentences.append(sentence)\n",
    "\n",
    "\treturn augmented_sentences\n",
    "\n",
    "def SR(sentence, alpha_sr, n_aug=9):\n",
    "\n",
    "\tsentence = get_only_chars(sentence)\n",
    "\twords = sentence.split(' ')\n",
    "\tnum_words = len(words)\n",
    "\n",
    "\taugmented_sentences = []\n",
    "\tn_sr = max(1, int(alpha_sr*num_words))\n",
    "\n",
    "\tfor _ in range(n_aug):\n",
    "\t\ta_words = synonym_replacement(words, n_sr)\n",
    "\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\taugmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences]\n",
    "\tshuffle(augmented_sentences)\n",
    "\n",
    "\taugmented_sentences.append(sentence)\n",
    "\n",
    "\treturn augmented_sentences\n",
    "\n",
    "def RI(sentence, alpha_ri, n_aug=9):\n",
    "\n",
    "\tsentence = get_only_chars(sentence)\n",
    "\twords = sentence.split(' ')\n",
    "\tnum_words = len(words)\n",
    "\n",
    "\taugmented_sentences = []\n",
    "\tn_ri = max(1, int(alpha_ri*num_words))\n",
    "\n",
    "\tfor _ in range(n_aug):\n",
    "\t\ta_words = random_addition(words, n_ri)\n",
    "\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\taugmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences]\n",
    "\tshuffle(augmented_sentences)\n",
    "\n",
    "\taugmented_sentences.append(sentence)\n",
    "\n",
    "\treturn augmented_sentences\n",
    "\n",
    "def RS(sentence, alpha_rs, n_aug=9):\n",
    "\n",
    "\tsentence = get_only_chars(sentence)\n",
    "\twords = sentence.split(' ')\n",
    "\tnum_words = len(words)\n",
    "\n",
    "\taugmented_sentences = []\n",
    "\tn_rs = max(1, int(alpha_rs*num_words))\n",
    "\n",
    "\tfor _ in range(n_aug):\n",
    "\t\ta_words = random_swap(words, n_rs)\n",
    "\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\taugmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences]\n",
    "\tshuffle(augmented_sentences)\n",
    "\n",
    "\taugmented_sentences.append(sentence)\n",
    "\n",
    "\treturn augmented_sentences\n",
    "\n",
    "def RD(sentence, alpha_rd, n_aug=9):\n",
    "\n",
    "\tsentence = get_only_chars(sentence)\n",
    "\twords = sentence.split(' ')\n",
    "\twords = [word for word in words if word is not '']\n",
    "\tnum_words = len(words)\n",
    "\n",
    "\taugmented_sentences = []\n",
    "\n",
    "\tfor _ in range(n_aug):\n",
    "\t\ta_words = random_deletion(words, alpha_rd)\n",
    "\t\taugmented_sentences.append(' '.join(a_words))\n",
    "\n",
    "\taugmented_sentences = [get_only_chars(sentence) for sentence in augmented_sentences]\n",
    "\tshuffle(augmented_sentences)\n",
    "\n",
    "\taugmented_sentences.append(sentence)\n",
    "\n",
    "\treturn augmented_sentences\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# Testing\n",
    "########################################################################\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\tline = 'Hi. My name is Jason. I’m a third-year computer science major at Dartmouth College, interested in deep learning and computer vision. My advisor is Saeed Hassanpour. I’m currently working on deep learning for lung cancer classification.'\n",
    "\n",
    "\n",
    "\n",
    "########################################################################\n",
    "# Sliding window\n",
    "# Slide a window of size w over the sentence with stride s\n",
    "# Returns a list of lists of words\n",
    "########################################################################\n",
    "\n",
    "# def sliding_window_sentences(words, w, s):\n",
    "# \twindows = []\n",
    "# \tfor i in range(0, len(words)-w+1, s):\n",
    "# \t\twindow = words[i:i+w]\n",
    "# \t\twindows.append(window)\n",
    "# \treturn windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12445,
     "status": "ok",
     "timestamp": 1732368731519,
     "user": {
      "displayName": "张殿",
      "userId": "10927409075531378782"
     },
     "user_tz": -480
    },
    "id": "S_GxmZXHpt06",
    "outputId": "833c567d-71b8-44c9-f0cf-e7a9034e50c3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "# 加载SST-2数据集\n",
    "data = pd.read_csv('/content/drive/MyDrive/599_Code/Dataset/Source_Dataset/SUBJ/train.csv')  # 替换为实际文件路径\n",
    "\n",
    "# EDA参数\n",
    "alpha_sr = 0.1  # 同义词替换比例\n",
    "alpha_ri = 0.1  # 随机插入比例\n",
    "alpha_rs = 0.1  # 随机交换比例\n",
    "p_rd = 0.1     # 随机删除概率\n",
    "num_aug = 4     # 每条数据生成的增强句子数量\n",
    "\n",
    "# 定义增强数据函数\n",
    "def augment_sst2_data(data, num_aug, alpha_sr, alpha_ri, alpha_rs, p_rd):\n",
    "    augmented_data = []\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        sentence, label = row['text'], row['label']\n",
    "\n",
    "        # 使用EDA技术进行数据增强\n",
    "        augmented_sentences = eda_4(sentence, alpha_sr, alpha_ri, alpha_rs, p_rd, num_aug)\n",
    "\n",
    "        # 保存增强后的句子和对应标签\n",
    "        for augmented_sentence in augmented_sentences:\n",
    "            augmented_data.append((augmented_sentence, label))\n",
    "\n",
    "    return pd.DataFrame(augmented_data, columns=['text', 'label'])\n",
    "\n",
    "# 增强数据集\n",
    "augmented_data = augment_sst2_data(data, num_aug, alpha_sr, alpha_ri, alpha_rs, p_rd)\n",
    "\n",
    "# 保存增强后的数据\n",
    "augmented_data.to_csv('/content/drive/MyDrive/599_Code/Dataset/Enhanced_Dataset/SUBJ/subj_data_augmented_by_EDA.csv', index=False)  # 替换为需要保存的文件路径\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMWi29aWRuy0HvopjEPQXfb",
   "mount_file_id": "1zir5jMkFADwIk1f_wVjQorDlenRwomnw",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
